# Simple Robotic Control with Neural Networks Imitation Learning

Robotic control with neural networks quickly gets into the weeds of neural networks. One might say "how hard could it be to drive a robot with a neural network?" Lets find out!

First off we will start by thinking about the data. When approaching machine learning design, data is always the place to start. Robots can have any number of sensors. So we will just assume we have a robot with GPS, Camera, and LIDAR (Light Detection and Ranging). The exact set of sensors is not all that important for this discussion. Beyond the sensors the robot also has controls. The inputs that make it move, which should be considered part of the data.

What do we wish to accomplish with the controlling of the robot? Give the huge variety in possible robotic designed let us once again assume that are robot is just a souped up RC car. And we just wish the robotic car to drive from point A to point B.

Now that we have a framework for discussion, we can begin to think about the design of the machine learning. The flow of data is fairly strait forward. We wish to transform the sensor input into drive commands.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/car-net.png" width="600" />

How do we obtain the data? Many popular robotic control systems are based on Reinforcement Learning. The network produces and action, then the quality of the action must be assess, based of the assessment the network is updated. For a reinforcement learning based approach to converge it often take far north of a millions iterations. How do you issue a million commands to your robot, evaluate the quality of the command in order for the network to update? Ends up that it is almost always done in a simulator. Simulators are fantaitic if you have one that is both fast and high fidelity. Yet most simulators are not fast and do not simulate with high enough fidelity to match the real world. If you are Nvidia then you can afford to make a simulator of sufficient quality and have a super computer to run it, but for the rest of us we likely can't afford the time to make an awesome simulator.

All hope is not lost. The beauty and the beast of software is there is always a different way to do anything. As the title of the article indicates we are intrudes some simple approaches to imitation learning.

Imitation learning is one way to do away with simulation. As the name suggest it will imitate an existing control algorithm. In this case we will use a human driver a the existing control algorithm. Now data collection is very strait forward. Just get a human to drive around for a long time and record all the sensor data along with the command the human is issuing (steering and acceleration). 

Now you have a simple data set that give a mapping from sensor data to commands. To be fair most imitation learning is formulated as reinforcment learing, but it dose not have to be. Inorder to keep things as simple as possible we will not look at it from the reinforcment learning persepctive. We will just formulat imtation learning as a supervised learning problem. This means we have an input and a label for the input. The network will be trained to map the input to the output.


The most basic approach is to create a leaner incoder that take the input and converts into the driving command. 

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```

This is an over simplified example. If you have image data coming from a camera then it make a lot of sense to use convolutional layers to learning features from the image. If you know the possible range for the output you might want to use an activation function that forces the output into the allowed range. We also have skipped over all the data preprocessing that should be done such as normilzation, resamping the collection rate, ect...

You also could easily make this a non linear Encoder. 

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation='relu'),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```


Yet there is an inherent limitation to using a Encoding approach to imitation learning. It dose not matter if the encoder is linear or nonlinear. This is the same problem that autoencoders run into. reversion to the mean. What dose reversion to the mean look like for robotic control? 

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/rmcar.png" width="600" />

Imagine your robot is approaching an obstacles. It can go left or right, both way are open. Which way should the neural network command the robot to go? Now comes reversion to the mean. Lets assume when the humane driver that we are learning to imitate was driving around, they drove around obstacles sometime to the right and some time to the left. What will the network learn? If the network commands the robot to go right but the label says to go left the loss will highly penalized the command. This mean the optimal for the network is to take the mean direction of all labels associated with avoiding objects. The mean option likely mean running directly into the object.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/carrm2.png" widht="600" />

This is not a good outcome. Reversion to the mean will cause all sorts of odd outcomes, as any input that has mutiple possible valid outputs will result in the mean. Doing the mean of the driving commands that are valid in a given cericumstance will produces a bad outcome.

Thankfully this has been addressed in a large variaty of machine learning domiains and is quite stratforward to adopt for driving our robot. 

What is the solution, it is the mighty GAN (Generative Adversarial Network). The GAN first got famouse for its ablity learn the distrubtion of a set of images. Learn the distrubtion of images is just fancy speak for genrate images that are similuer to a set of training images.


<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/faces.jpg" width="600" />


Ends up GANs are good at learning lots of different distributions. This includes the distribution over driving commands to a robot. There are lots of ways people describe GANs, but the part we care about is that GANs break the direct relationship between inputs and output labels. This decoupling of the inputs and labels solves prevents the network from reverting to the mean.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/GAN.jpg" width="600" />

This is what are GAN structure will look like. This is not quite a vanilla GAN, it actually a conditional GAN. The Encoder / Generator is almost exactly the same as the encoder above. The only differences are the addition of the z input and the fact that the labels are not used directly to train it. z is a noise vector (random values), which is required to provide stochasticity to the generator. Even give the decoupling of the labels a neural network will still always produce the same output given the same input. Adding a noise vector allow the generator to produce different drive commands given identical sensor data. This is good, because we know that often time there exist may valid drive commands.

A GAN has an addition network called the discriminator. The job of the discriminator is to classify if the input came from the Generator or the actual dataset. That might seem a bit confusing, so lets run through an example of the steps involved in training.

1. Sample a random vector z.
2. Sample a random sensor data example X1.
3. Input the sensor data plus the noise vector z into the generator, outputting a drive command C1.
4. Sample another random sensor data example and drive command label from the real dataset (X2, C2).
5. Give the discriminator (X2, C2) and (X1, C1).
6. The discriminator should attempt to predict that (X2, C2) came from the real collected training data and that (X1, C1) was generated.

Now lets think about the loss function for a sec. The discriminator has a loss function focused on classifying if samples are from the Real distribution or from the generated distribution. We want the generator to output valid drive commands given sensor data, which means we want its outputs to be capable of fooling the discriminator. This means we can use the inverses loss from the discriminator to update the generator. This is where the name Generative Adversarial Networks comes from. The generator is the adversary of the discriminator. The discriminator is constantly attempting to improves its ability to differentiate between real and generated. While the generator is attempting to match the distribution of the real samples so closely that the discriminator cant tell the difference.

This learning approach breaks the direct relation ship between the labels and the output of the encoder/generator. Reframing the learning task as learning a distribution rather then a mapping. This solves the problem we faced with the encoder reverting to the mean.

Ready to talk Code? Me too. There is a fair amount of subtlety to training a GAN. We will not cover it all here but I will try to point out as much as possible while giving the broad strokes.


The Network defeniton in really quite simple. 
```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation='relu'),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```




