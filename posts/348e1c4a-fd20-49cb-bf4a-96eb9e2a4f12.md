# Simple Robotic Control with Neural Networks Imitation Learning

Robotic control with neural networks quickly gets into the weeds of neural networks. One might say "how hard could it be to drive a robot with a neural network?" Lets find out!

First off we will start by thinking about the data. When approaching machine learning design, data is always the place to start. Robots can have any number of sensors. So we will just assume we have a robot with GPS, Camera, and LIDAR (Light Detection and Ranging). The exact set of sensors is not all that important for this discussion. Beyond the sensors the robot also has controls. The inputs that make it move, which should be considered part of the data.

What do we wish to accomplish with the controlling of the robot? Give the huge variety in possible robotic designed let us once again assume that are robot is just a souped up RC car. And we just wish the robotic car to drive from point A to point B.

Now that we have a framework for discussion, we can begin to think about the design of the machine learning. The flow of data is fairly strait forward. We wish to transform the sensor input into drive commands.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/car-net.png" width="600" />

How do we obtain the data? Many popular robotic control systems are based on Reinforcement Learning. The network produces and action, then the quality of the action must be assess, based of the assessment the network is updated. For a reinforcement learning based approach to converge it often take far north of a millions iterations. How do you issue a million commands to your robot, evaluate the quality of the command in order for the network to update? Ends up that it is almost always done in a simulator. Simulators are fantaitic if you have one that is both fast and high fidelity. Yet most simulators are not fast and do not simulate with high enough fidelity to match the real world. If you are Nvidia then you can afford to make a simulator of sufficient quality and have a super computer to run it, but for the rest of us we likely can't afford the time to make an awesome simulator.

All hope is not lost. The beauty and the beast of software is there is always a different way to do anything. As the title of the article indicates we are intrudes some simple approaches to imitation learning.

Imitation learning is one way to do away with simulation. As the name suggest it will imitate an existing control algorithm. In this case we will use a human driver a the existing control algorithm. Now data collection is very strait forward. Just get a human to drive around for a long time and record all the sensor data along with the command the human is issuing (steering and acceleration). 

Now you have a simple data set that give a mapping from sensor data to commands. To be fair most imitation learning is formulated as reinforcment learing, but it dose not have to be. Inorder to keep things as simple as possible we will not look at it from the reinforcment learning persepctive. We will just formulat imtation learning as a supervised learning problem. This means we have an input and a label for the input. The network will be trained to map the input to the output.


The most basic approach is to create a leaner incoder that take the input and converts into the driving command. 

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```

This is an over simplified example. If you have image data coming from a camera then it make a lot of sense to use convolutional layers to learning features from the image. If you know the possible range for the output you might want to use an activation function that forces the output into the allowed range. We also have skipped over all the data preprocessing that should be done such as normilzation, resamping the collection rate, ect...

You also could easily make this a non linear Encoder. 

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation='relu'),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```


Yet there is an inherent limitation to using a Encoding approach to imitation learning. It dose not matter if the encoder is linear or nonlinear. This is the same problem that autoencoders run into. reversion to the mean. What dose reversion to the mean look like for robotic control? 

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/rmcar.png" width=600>

Imagine your robot is approaching an obstacles. It can go left or right, both way are open. Which way should the neural network command the robot to go? Now comes reversion to the mean. Lets assume when the humane driver that we are learning to imitate was driving around, they drove around obstacles sometime to the right and some time to the left. What will the network learn? If the network commands the robot to go right but the label says to go left the loss will highly penalized the command. This mean the optimal for the network is to take the mean direction of all labels assocated with avoiding objects. The mean option likely mean running directly into the object. 

