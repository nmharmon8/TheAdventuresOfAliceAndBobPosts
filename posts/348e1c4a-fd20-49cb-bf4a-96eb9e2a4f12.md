# Simple Robotic Control with Neural Networks Imitation Learning

Created: 03/12/2022

Robotic control with neural networks quickly gets into the weeds of neural networks. How hard could it be to drive a robot with a neural network? Lets find out!

First off we will start by thinking about the data. When approaching machine learning design, data is always the place to start. Robots can have any number of sensors. The exact set of sensors is not very important for this discussion. Beyond the sensors, the robot also has controls. The inputs, which make it move, should be considered part of the data.

What do we wish to accomplish with controlling the robot? Given the huge variety in possible robotic designs, let us assume that our robot is a souped up RC car. We just wish the robotic car to drive from point A to point B.

Now that we have a framework for discussion, we can begin to think about the machine learning design. The flow of data is fairly straight forward. We wish to transform the sensor input into drive commands.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/car-net.png" width="600" />

How do we obtain the data? Many popular robotic control systems are based on Reinforcement Learning. The network produces an action, then the quality of the action must be assessed, based on the assessment the network is updated. For a reinforcement learning based approach, converging it often takes far north of millions of iterations. How do you issue a million commands to your robot and evaluate the quality of the command in order for the network to update? As it ends up, it is almost always done in a simulator. Simulators are fantastic if you have one that is both fast and high fidelity. Most simulators are not adequately fast and do not simulate with a high enough fidelity to match the real world. If you are Nvidia, then you can afford to make a simulator of sufficient quality and have a super computer to run it on. For the rest of us, we likely can not afford the time to make an awesome simulator.

All hope is not lost. The beauty and the beast of software is that there is always a different way to do anything. As the title of the article indicates we are discussing some simple approaches to imitation learning.

Imitation learning is one way to do away with simulation. As the name suggest it will imitate an existing control algorithm. In this case we will use a human driver a the existing control algorithm. Data collection is very straight forward. Get a human to drive around for a long time and record all the sensor data along with the commands the human is issuing (steering and acceleration). 

Now you have a simple dataset that gives a mapping from sensor data to commands. To be fair most imitation learning is formulated as reinforcement learning, but it does not have to be. In order to keep things as simple as possible, we will not look at it from the reinforcement learning perspective, we will just formulate imitation learning as a supervised learning problem. This means we have an input and a label for the input. The network will be trained to map the input to the output.


The most basic approach is to create a linear encoder that takes the input and converts into the driving command. 

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```

This is an over simplified example. If you have image data coming from a camera then it makes sense to use convolutional layers to learning features from the image. If you know the possible range for the output you might want to use an activation function that forces the output into the allowed range. We have skipped over all the data preprocessing that should be done; normalization, resampling the collection rate, ect...

You also could easily make this a nonlinear Encoder. 

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    self.encoder = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation='relu'),
      layers.Dense(number_of_commands),
    ])
   

  def call(self, x):
    commands = self.encoder(x)
    return commands
```


There is an inherent limitation to using an encoding approach to imitation learning. It does not matter if the encoder is linear or nonlinear. This is the same problem that autoencoders run into which is, reversion to the mean. What does reversion to the mean look like for robotic control? 

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/rmcar.png" width="600" />

Imagine your robot is approaching an obstacle. It can go left or right; both ways are open. Which way should the neural network command the robot to go? Lets assume with a humane driver, that the network has learned to imitate, was driving around they drove around obstacles sometime on the right and some time on the left. What will the network learn? If the network commands the robot to go right but the label says to go left the loss will highly penalized the command. This mean the optimal for the network is to take the mean direction of all labels associated with avoiding objects. The mean option likely mean running directly into the object.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/carrm2.png" width="600" />

This is not a good outcome. Reversion to the mean will cause all sorts of odd outcomes, as any input that has multiple possible valid outputs will result in the mean of the valid options.

Thankfully this has been addressed in a large variety of machine learning domains and is quite straightforward to adopt for driving our robot. 

What is the solution? The mighty GAN (Generative Adversarial Network). The GAN first got famous for its ability learn the distribution of a set of images. Learning the distribution of images is fancy speak for generate images that are similar to a set of training images.


<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/faces.jpg" width="600" />


Ends up GANs are good at learning lots of different distributions. This includes the distribution over driving commands to a robot. There are lots of ways people describe GANs, but the part we care about is that GANs break the direct relationship between inputs and output labels. This decoupling of the inputs and labels solves reverting to the mean.

<img src="https://raw.githubusercontent.com/nmharmon8/TheAdventuresOfAliceAndBobPosts/main/posts/post_assets/robotic/GAN.jpg" width="600" />

This is what are GAN structure will look like. This is not quite a vanilla GAN, it actually a conditional GAN (more on that later). The Encoder / Generator is almost exactly the same as the encoder above. The only differences are the addition of the z input and the fact that the labels are not used directly to train it. z is a noise vector (random values), which is required to provide stochasticity to the generator. Even give the decoupling of the labels, a neural network will still always produce the same output given the same input. Adding a noise vector allow the generator to produce different drive commands given identical sensor data. This is good, because we know that often time there exist may valid drive commands.

A GAN has an addition network called the discriminator. The job of the discriminator is to classify if the input came from the Generator or the actual dataset. That might seem a bit confusing, so lets run through an example of the steps involved in training.

1. Sample a random vector z.
2. Sample a random sensor data example X1.
3. Input the sensor data plus the noise vector z into the generator, outputting a drive command C1.
4. Sample another random sensor data example and drive command label from the real dataset (X2, C2).
5. Give the discriminator (X2, C2) and (X1, C1).
6. The discriminator should attempt to predict that (X2, C2) came from the real collected training data and that (X1, C1) was generated.

Now lets think about the loss function for a sec. The discriminator has a loss function focused on classifying if samples are from the real distribution or from the generated distribution. We want the generator to output valid drive commands given sensor data, which means we want its outputs to be capable of fooling the discriminator. This means we can use the inverses loss from the discriminator to update the generator. This is where the name Generative Adversarial Networks comes from. The generator is the adversary of the discriminator. The discriminator is constantly attempting to improves its ability to differentiate between real and generated. While the generator is attempting to match the distribution of the real samples so closely that the discriminator cant tell the difference.

This learning approach breaks the direct relation ship between the labels and the output of the encoder/generator. Reframing the learning task as learning a distribution rather then a mapping. This solves the problem we faced with the encoder reverting to the mean.

Ready to talk Code? I know I am. There is a fair amount of subtlety to training a GAN. We will not cover it all here but I will try to point out as much as possible while giving the broad strokes.


The Network definition in really quite simple. This GAN is actually simpler then an image GAN because the outputs is low dimensional (just drive commands). This will also make convergences much easier to reach. Convergence for a GAN is when the discriminator and generator reach Nash equilibrium [(or not, ha ha got you)](https://arxiv.org/abs/2002.09124).

```python
class LinearEncoder(Model):
  def __init__(self, number_of_commands):
    super(LinearEncoder, self).__init__()
    self.input_size = input_size
    self.number_of_commands =number_of_commands
    
    
    self.generator = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation='relu'),
      layers.Dense(number_of_commands),
    ])
    
    self.discriminator = tf.keras.Sequential([
      layers.Flatten(),
      layers.Dense(128, activation='relu'),
      layers.Dense(1),
    ])
  
```

Note that the discriminator just has one output and it is just 1 or 0 for real or generated. In practice you have to keep the discriminator and generator somewhat equivalent in capability or one or the other will dominate and the gradient will vanish. Sounds magical, vanishing gradients ooo, but it just annoying and solvable [a few common problems](https://developers.google.com/machine-learning/gan/problems). What dose somewhat equivalent in capability mean? Its relative to the problem. Could I be any less helpful, why yes I can. 

If this is as poorly written as I think then you probably have not guessed that the difficulty of GANs is not a complicated network topology like Transformers, rather it is complicated training procedures.

At this point I am going to steal some code for [tensorflow.org](https://www.tensorflow.org/tutorials/generative/dcgan) and adopt to the above network.

```python
# This method returns a helper function to compute cross entropy loss
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

```

So the generator loss is simple. The generator want the discriminator to think its samples were real. So it just computes loss using 1 as the label for all the generated samples. This is the inverse of what the discriminator loss will be for the generator samples. 

Now look at the discriminator loss function. If you look at `fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)` calculating the inverse of the generator using 0 rather then 1. So all the discriminator loss is doing is assigning the label 1 for samples from the real dataset and 0 for samples from the generator.

The generator is using the output of the discriminator to update how it generates samples. Essentially the gradent from the discriminator explains how it knows the sample was not real so the generator can update its weights to better match the distribution of real samples.

Now for the dreaded training loop:

```python
EPOCHS = 50
noise_dim = 100


seed = tf.random.normal([num_examples_to_generate, noise_dim])

network = LinearEncoder(number_of_commands=3)

# Notice the use of `tf.function`
# This annotation causes the function to be "compiled".
@tf.function
def train_step(sensor_data_and_labels, sensor_data):
    
    #The sensor_data_and_labels is randomly samples separately from sensor_data. e.g not the same sampels
    
    noise = tf.random.normal([BATCH_SIZE, noise_dim])
    
    generator_input = tf.concat([noise, sensor_data], axis=1)
    
    

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_drive_commands = network.generator(generator_input, training=True)
      conditioned_drive_commands = tf.concat([sensor_data, generated_drive_commands], axis=1)
    
      fake_output = network.discriminator(conditioned_drive_commands, training=True)
      real_output = network.discriminator(sensor_data_and_labels, training=True)

      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)

    #Only apply the loss to the correct networks
    gradients_of_generator = gen_tape.gradient(gen_loss, network.generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, network.discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, network.generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, network.discriminator.trainable_variables))

```

There is a lot is that training loop. Fist as was mentioned earlier this is a conditional GAN. So the generator takes in sensor data and a noise vector, vs a traditional GAN would take in only a noise vector. In this case a drive command is meaningless except in the context of sensor data. For images it is different, because an image can stand on its own without any other data. The next implication of a conditional GAN is that the discriminator can not tell if a drive command is realistic without sensor data, so it also will take in sensor data and drive commands. 

Inputting the data pairs (sensor_data, drive_command) to the discriminator depends on the types of inputs. If they are all vectors then you can just concat them all into one big vector. If you have images in your sensor data they you may need to have multiple heads on the discriminator.

You can now see the flow of the training:

1. Run the generator
2. Run the discriminator on the output of the generator, create fake outputs
3. Run the discriminator on the real data from the dataset, create real outputs
4. Compute the loss for the generator
5. Compute the loss for the discriminator
6. Apply the computed loss to the respective networks

Note that applying the loss is done using only the trainable variables for the generator and discriminator separately. The discriminator network is necessary for computing loss for the generator, but that loss must only be applied to the generator. Remember that the discriminator and generator losses are close to inverses so they would cancel each other out if you accidentally applied them to both networks.

Once the generator is trained you can trow out the discriminator and just use the generator to drive your robot. However the discriminator can come in handy for detecting if the robot dose not actually know how to handle a specific sensor data input. If you only use the generator the robot will keep driving even if the input is  garbage or just something it has never seen before. But if you use the discriminator it will let you know that the robot is off the rails and you should takeover.  

Congratulations, hopefully you have a vision for how to use a GAN based approach for imitation learning.



